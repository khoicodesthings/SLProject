\section*{Intended Models and Hyposthesis}

\subsection*{Models}
Naive Bayes is a group of probabilistic machine learning
algorithms based on applying Bayes' theorem with the assumption of independence between features. This assumes
that the presence or absence of one feature does not affect
the presence or absence of any other feature within a class,
and calculates the probabilities of each class label given the
values of the features from the input. The class with the
highest probability is then selected as the predicted class
label for the given input.

Logistic Regression is a statistical machine learning algorithm predicting the probability of a target variable by
fitting a logistic function to the input features. This optimizes the parameters of the logistic function by minimizing
the cost function, which measures the difference between
the predicted probabilities and the actual class labels.

Decision Tree is a machine learning algorithm to predict
the class of an input based on its features. This algorithm
partitions the input space into increasingly smaller regions
based on the values of the input features by selecting the
features and test conditions that best separate the training
data into the different classes. This process is repeated
recursively to create a tree-like structure until a final decision
is made.

Random Forest is an ensemble learning method that combines multiple decision trees to improve the performance
and reduce overfitting. Each decision tree is constructed by
randomly selecting a set of features and samples from the
training set. This randomization helps to make a model less
sensitive to noise and outliers in the data, and outperforms
other algorithms.

\subsection*{Hypotheses}
Our first hypothesis is that our supervised learning models
will be able to classified these songs as likes or dislikes with
at least 90\% accuracy.

Our second hypothesis is that out of the four chosen algorithms; Random Forest will give us the best performance;
followed by Decision Tree, then Logistic Regression, and
finally Naive Bayes. We will evaluate each algorithm's performance by comparing the value of Precision, Recall, and
produce ROC curves for each algorithm.